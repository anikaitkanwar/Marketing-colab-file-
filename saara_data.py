# -*- coding: utf-8 -*-
def run_pipeline(
    report_date_input: str,
    delay: int,
    user: str,
    spreadsheet_url: str
):
    global report_date
    global start_date
    global User
    global spreadSheet_URL

    User = user
    spreadSheet_URL = spreadsheet_url

    from datetime import datetime, timedelta
    from pytz import timezone

    report_date = datetime.strptime(report_date_input, "%Y-%m-%d")
    start_date = (report_date - timedelta(days=delay)).strftime("%Y-%m-%d 18:30:00")

    ind_time = datetime.now(timezone("Asia/Kolkata")).strftime('%Y-%m-%d %H:%M:%S.%f')
    print('last used by', User, 'at', ind_time)

    # ---- CALL YOUR EXISTING LOGIC HERE ----
    # Example:
    # query = generate_query(start_date, report_date_input, report_date_input)
    # fetch data
    # write to google sheet

    print("Pipeline completed successfully")

"""Saara_data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UJfX7u9GHfI2qJvN6vlNyLdLrZ-Nzhfo
"""

report_date_input = "2026-01-14"       # end date of the tenure
delay = 90
Category = 'Finance'
worksheet_name = 'Last 3 Months'
spreadSheet_URL = 'https://docs.google.com/spreadsheets/d/13mH0fgE6tdWmR6PwoVvtqRyudvW2nAJl-Af8XQa3GG8/edit?usp=sharing'
User = "Anikait"

# YE NIICHE KI LINKS MAT CHHEDNA - when you want to run the script, niiche wale sab me start me '#' daal dena and remove the '#' from the above link. kuch puchna hai to @Harshit
# spreadSheet_URL = 'https://docs.google.com/spreadsheets/d/1vlaWx5A4LT6p_sZp21hibFHeWuwqka8MUM_01zhyxzM/edit?gid=294098006#gid=294098006' # tradewise may 2025 projections marketing
# spreadSheet_URL = 'https://docs.google.com/spreadsheets/d/1LppZv-MI6qm-fIzndlYNAyI6X4a_Zc0e3A2CcB4g3Pg/edit?gid=1826985705#gid=1826985705' # TW Dec Mktg MTD Targets
# spreadSheet_URL = 'https://docs.google.com/spreadsheets/d/1mZ8274jh9FyUu8sC-qztcKGZeL8MHCLu3sf3Q7Z_HSM/edit?gid=299841015#gid=299841015' # Marketing M0 and W0 (Campaign)
# spreadSheet_URL = 'https://docs.google.com/spreadsheets/d/1aWQbutBtBtHec5_Yjzb1aYe33LQYxM5fd_G-qaERFyQ/edit?gid=2145741827#gid=2145741827' # WOW Marketing | Chinmay
# spreadSheet_URL = 'https://docs.google.com/spreadsheets/d/1-W3AVMmGRk5YfULbzL5uPes3wRlSINVln-8NyL6geEE/edit?gid=1170975117#gid=1170975117' # Dump | Marketing
# spreadSheet_URL = 'https://docs.google.com/spreadsheets/d/1nR2o0E611t28aOdNS2cqfNtBaari5TXi2avLXQxbSW8/edit?usp=sharing' # Tradewise | Ananya
# spreadSheet_URL = 'https://docs.google.com/spreadsheets/d/1N3QHGY6hUuqnS0dUuV7Zse-INhkRe12oAT87S8vZafU/edit?usp=sharing' # Daily Data | Ananya
# spreadSheet_URL = 'https://docs.google.com/spreadsheets/d/1echJQoHOjx2AKBGQHfqMW85bGjHmkNbt2KWnyAeeolc/edit?usp=sharing' # Harshit (Mastersheet Astro)
# spreadSheet_URL = 'https://docs.google.com/spreadsheets/d/1eQdn6m-sW_S6kOCOA2_20t7xrcFA97MqVXCdZKA1Q7A/edit?usp=sharing' # astroji
# give editor access to this id before you paste the url of sheet here
# ID---->>>> class-spend@dailyclassspend.iam.gserviceaccount.com
# THE SHEET NAME YOU PROVIDED HERE MUST BE UNIQUE I.E THE SPREADSHEET MUST NOT CONTAIN THE SHEET WITH THE SAME NAME
# SELECT RUN ALL FROM RUNTIME
from pytz import timezone
from datetime import datetime
ind_time = datetime.now(timezone("Asia/Kolkata")).strftime('%Y-%m-%d %H:%M:%S.%f')
print('last used by',User,'at',ind_time)

from datetime import datetime, timedelta,date
report_date = datetime.strptime(report_date_input, "%Y-%m-%d")
start_date = (report_date - timedelta(days=delay)).strftime("%Y-%m-%d 18:30:00")
start_date

import psycopg2
import pandas as pd
from datetime import datetime, timedelta,date
def execute_query(host, database, user, password, port, query):
    conn = None
    cur = None

    try:
        conn = psycopg2.connect(
            host=host,
            database=database,
            user=user,
            password=password,
            port=port
        )
        cur = conn.cursor()

        # Execute the query
        cur.execute(query)
        results = cur.fetchall()
        return results

    except (Exception, psycopg2.Error) as error:
        print("Error while executing query:", error)
        return None

    finally:
        if conn:
            if cur:
                cur.close()
            conn.close()
            print("PostgreSQL connection is closed")


def generate_query(start_date, end_date, report_date):
    query = f"""
    With Leads as (SELECT DISTINCT
    cat."name", mc."title", l."source", COALESCE(l."comment", l."payload"->>'ad_id') AS "comment", COUNT(DISTINCT l."userId") AS "Users",COUNT(DISTINCT l."id") AS "Leads"
FROM "Leads" l
JOIN "MasterClassSlots" mcs ON l."masterclassSlotId" = mcs."id"
JOIN "MasterClass" mc ON mc."id" = mcs."masterClassId"
JOIN "Bootcamp" b ON mc."bootcampId" = b."id"
JOIN "User" u ON b."teacherId" = u."id"
JOIN "Categories" cat ON u."categoryId" = cat."id"
WHERE l."createdAt" BETWEEN '{start_date}' AND '{end_date}'
    AND l."source" IN ('facebook', 'google', 'Taboola','zepto','swiggy','gpay','zomato','cred','paytm','twitter')
GROUP BY l."source", COALESCE(l."comment", l."payload"->>'ad_id'), mc."title", cat."name"),

Joins as (SELECT DISTINCT
    cat."name", mc."title", l."source", COALESCE(l."comment", l."payload"->>'ad_id') AS "comment", COUNT(DISTINCT l."userId") AS "Joins",Count(distinct(concat(l."userId",mca."meetingId"))) as total_joins
FROM "Leads" l
JOIN "MasterClassSlots" mcs ON l."masterclassSlotId" = mcs."id"
JOIN "MasterClass" mc ON mc."id" = mcs."masterClassId"
JOIN "Bootcamp" b ON mc."bootcampId" = b."id"
JOIN "User" u ON b."teacherId" = u."id"
JOIN "Categories" cat ON u."categoryId" = cat."id"
Join "MasterclassAttendees" mca on mca."userId" = l."userId"
WHERE l."createdAt" BETWEEN '{start_date}' AND '{end_date}'
        and mca."createdAt" BETWEEN l."createdAt" AND '{end_date}'
    AND l."source" IN ('facebook', 'google', 'Taboola','zepto','swiggy','gpay','zomato','cred','paytm','twitter')
GROUP BY l."source", COALESCE(l."comment", l."payload"->>'ad_id'), mc."title", cat."name"),

rev as (with payss as (SELECT DISTINCT
    cat."name", mc."title", l."source", COALESCE(l."comment", l."payload"->>'ad_id') AS "comment", pi."id", pi."amount"
FROM "Leads" l
JOIN "MasterClassSlots" mcs ON l."masterclassSlotId" = mcs."id"
JOIN "MasterClass" mc ON mc."id" = mcs."masterClassId"
JOIN "Bootcamp" b ON mc."bootcampId" = b."id"
JOIN "User" u ON b."teacherId" = u."id"
JOIN "Categories" cat ON u."categoryId" = cat."id"
Join "PaymentIntent" pi on pi."userId" = l."userId"
WHERE l."createdAt" BETWEEN '{start_date}' AND '{end_date}'
        and pi."createdAt" BETWEEN l."createdAt" - interval '2 mins' AND '{end_date}'
        and pi."status" = '1'
    AND l."source" IN ('facebook', 'google', 'Taboola','zepto','swiggy','gpay','zomato','cred','paytm','twitter'))

select "name", "title", "source", "comment",  sum(cast ("amount" as numeric)/100) as "Total_Revenue"
from "payss"
group by "name", "title", "source", "comment"),


sales as ( SELECT DISTINCT
    cat."name", mc."title", l."source", COALESCE(l."comment", l."payload"->>'ad_id') AS "comment", count(distinct pi."id" ) as "Sales_Count"
FROM "Leads" l
JOIN "MasterClassSlots" mcs ON l."masterclassSlotId" = mcs."id"
JOIN "MasterClass" mc ON mc."id" = mcs."masterClassId"
JOIN "Bootcamp" b ON mc."bootcampId" = b."id"
JOIN "User" u ON b."teacherId" = u."id"
JOIN "Categories" cat ON u."categoryId" = cat."id"
Join "PaymentIntent" pi on pi."userId" = l."userId"
WHERE l."createdAt" BETWEEN '{start_date}' AND '{end_date}'
        and pi."createdAt" BETWEEN l."createdAt" AND '{end_date}'
        and round (cast (pi."amount" as numeric)) > 29900
        and pi."status" = '1'
    AND l."source" IN ('facebook', 'google','Taboola','zepto','swiggy','gpay','zomato','cred','paytm','twitter')
GROUP BY l."source", COALESCE(l."comment", l."payload"->>'ad_id'), mc."title", cat."name")


SELECT
    Leads."name", Leads."title", Leads."source",  Leads."comment", Leads."Users",Leads."Leads", Joins."Joins",Joins."total_joins", rev."Total_Revenue", sales."Sales_Count"
FROM
    Leads
LEFT JOIN
    Joins ON Leads."name" = Joins."name"
    AND Leads."title" = Joins."title"
    AND Leads."source" = Joins."source"
    AND Leads."comment" = Joins."comment"
LEFT JOIN
    rev ON Leads."name" = rev."name"
    AND Leads."title" = rev."title"
    AND Leads."source" = rev."source"
    AND Leads."comment" = rev."comment"
LEFT JOIN
    sales ON Leads."name" = sales."name"
    AND Leads."title" = sales."title"
    AND Leads."source" = sales."source"
    AND Leads."comment" = sales."comment";
    """
    return query

host = "data-analysis-db-ro-postgresql-blr1-73858-do-user-13062511-0.m.db.ondigitalocean.com"
database = "test"
user = "doadmin"
password = "AVNS_zuOg83f71JBINpat9pi"
port = "25060"


report_date = datetime.strptime(report_date_input, "%Y-%m-%d")
start_date = (report_date - timedelta(days=delay)).strftime("%Y-%m-%d 18:30:00")
end_date = report_date.strftime("%Y-%m-%d 18:30:00")
query = generate_query(start_date, end_date, report_date.strftime("%Y-%m-%d"))
results_day_before_yesterday = execute_query(host, database, user, password, port, query)
results = execute_query(host, database, user, password, port, query)

df = pd.DataFrame(results)
df.columns = ['Category','Title','Source','Comment','Users',"Leads",'Joins','Total Joins','Revenue','Converted']

df['Revenue'] = pd.to_numeric(df['Revenue'], errors='coerce')
df['Converted'] = pd.to_numeric(df['Converted'], errors='coerce')
df['Leads'] = pd.to_numeric(df['Leads'], errors='coerce')

df['Joins'] = df['Joins'].fillna(0).astype(int)
df['Users'] = df['Users'].astype(int)

df

df_grouped = df.groupby(['Comment', 'Source', 'Category','Title'], as_index=False).agg(
    Users=('Users', 'sum'),
    Leads=('Leads', 'sum'),
    Joins = ('Joins','sum'),
    Revenue=('Revenue', 'sum'),
    Converted=('Converted', 'sum'))
df_grouped.sort_values(by='Revenue' ,ascending=False)

def overall_spend(report_date, starting_report_date):
    query2 = f'''
     SELECT
    a.platform,
    a."campaignName",
    a."adName",
    a."adsetName",
    a."landingUrl",
    "adId",
    a.spend AS spends,
    a.impressions AS impressions,

        CASE
            WHEN a.platform = 'facebook' THEN a."inlineLinkClicks"
            ELSE a.clicks
        END
     AS clicks
FROM "AdStats" a
WHERE a."reportDate" BETWEEN '{starting_report_date}' AND '{report_date}'
GROUP BY
    a.platform,
    a."campaignName",
    a."adName",
    a."adsetName",
    a."landingUrl",
    a."adId",
    a.spend,
    a.impressions,
    a."inlineLinkClicks",
    a.clicks


        '''
    return query2
start_report_date = (report_date - timedelta(days = delay-1))
query2 = overall_spend(report_date,start_report_date)
google_spending = execute_query(host, database, user, password, port, query2)
spending_df= pd.DataFrame(google_spending)
# Rename columns
spending_df.columns = ['Platform', 'campaignName', 'adName', 'adsetName', 'landingUrl', 'adId','spend', 'impressions', 'clicks']
cols = ['spend', 'impressions', 'clicks']
spending_df[cols] = spending_df[cols].where(spending_df[cols] >= 0, 1)
# Group, aggregate, and reset index for easier sorting
grouped_df = spending_df.groupby(['Platform', 'campaignName', 'adName', 'adsetName'])[['spend', 'impressions', 'clicks']].sum().reset_index()

# Sort by spend ascending
grouped_df = grouped_df.sort_values(by='spend', ascending=True)
spending_df

adId_df = spending_df.groupby('adId').agg(
   { 'spend':'sum',
    'impressions':'sum',
    'clicks': 'sum'}
).reset_index()

rev_funnel = df_grouped.copy()
spend_funnel = spending_df.copy()

rev_funnel['Comment'] = rev_funnel['Comment'].str.replace(' ', '+', regex=False)
rev_funnel['Comment'] = rev_funnel['Comment'].str.replace('$', '', regex=False)

from urllib.parse import unquote
def unquoted_url(url):
    return unquote(url)
spend_funnel['landingUrl'] = spend_funnel['landingUrl'].apply(unquoted_url)
spend_funnel =  spend_funnel.replace(-1, 0)
spend_funnel['campaignName'] = spend_funnel['campaignName'].str.replace('$', '', regex=False)
spend_funnel['adName'] = spend_funnel['adName'].str.replace('$', '', regex=False)
spend_funnel['adsetName'] = spend_funnel['adsetName'].str.replace('$', '', regex=False)
spend_funnel['landingUrl'] = spend_funnel['landingUrl'].str.replace('$', '', regex=False)

import urllib.parse
def extract_comment_raw(url):
    try:
        qs = urllib.parse.urlparse(url).query
        for part in qs.split('&'):
            if part.startswith('comment='):
                return part[len('comment='):]  # raw, unparsed
        return None
    except:
        return None

spend_funnel['landing_para'] = spend_funnel['landingUrl'].apply(extract_comment_raw)
spend_funnel['landing_para'] = spend_funnel['landing_para'].str.replace('$', '', regex=False)
spend_funnel = spend_funnel.fillna(0)
spend_funnel[spend_funnel['adsetName'].str.contains('&',case=False,na = False)]['adsetName'].tolist()

landing_para_df = spend_funnel.groupby(['landing_para','campaignName','adName','adsetName']).agg(
   { 'spend':'sum',
    'impressions':'sum',
    'clicks': 'sum'}
).reset_index()

landing_para_df

rev_funnel['Comment'] = rev_funnel['Comment'].str.split('@@').str[0]
rev_funnel = rev_funnel.groupby(['Category','Title','Source','Comment'],    as_index=False
).sum(numeric_only=True)
grouped_spend = spend_funnel.groupby(
    ['adName', 'Platform', 'adsetName','campaignName'],
    as_index=False
).sum(['spend','impressions','clicks'])

# grouped_spend[grouped_spend['landing_para'].str.contains('MaheshPurchaseLookalike1%', na=False)]

grouped_spend_df = grouped_spend.copy()
grouped_spend_df['key2'] = grouped_spend_df['campaignName'].astype(str) + grouped_spend_df['adsetName'].astype(str) + grouped_spend_df['adName'].astype(str)

matched = pd.merge(rev_funnel,grouped_spend_df,left_on='Comment',right_on= 'key2',how = 'left')

s1 = matched[~(matched['spend'].isna())]

s1[(s1['Source']=='facebook')&(s1['Category']=='Finance')]['spend'].sum()

unmatched = matched[(matched['spend'].isna())]

unmatched.dropna(axis=1, how='all', inplace=True)

matched = pd.merge(unmatched,adId_df,left_on = 'Comment',right_on = 'adId',how = 'left')

s2 = matched[~(matched['spend'].isna())]

s2['campaignName'] = 0
s2['adsetName'] = 0
s2['adName'] = 0

s2

unmatched = matched[(matched['spend'].isna())]

unmatched.dropna(axis=1, how='all', inplace=True)

matched = pd.merge(unmatched,spending_df,left_on = 'Comment',right_on = 'adName',how = 'left')

s3 = matched[~(matched['spend'].isna())]

unmatched = matched[(matched['spend'].isna())]

unmatched.dropna(axis=1, how='all', inplace=True)

unmatched

matched = pd.merge(unmatched,landing_para_df,left_on = 'Comment',right_on = 'landing_para',how = 'left')

matched

s4 = matched[~(matched['spend'].isna())]

s4

unmatched = matched[(matched['spend'].isna())]

unmatched.dropna(axis=1, how='all', inplace=True)

grouped_spend_df['key1'] = grouped_spend_df['campaignName'].astype(str) + grouped_spend_df['adName'].astype(str)

matched = pd.merge(unmatched,grouped_spend_df,left_on = 'Comment',right_on = 'key1',how = 'left')

s5 = matched[~(matched['spend'].isna())]

unmatched = matched[(matched['spend'].isna())]

columns = ['Category','Title','Source','Comment','campaignName','adsetName','adName','spend','impressions','clicks','Users','Leads','Joins','Converted','Revenue']
s1 = s1[columns]
s2 = s2[columns]
s3 = s3[columns]
s4 = s4[columns]
s5 = s5[columns]

matched = pd.concat([s1,s2,s3,s4,s5],axis = 0)

matched[(matched['Source'] == 'facebook') & (matched['Category'] == 'Finance') ]['spend'].sum()

unmatched[(unmatched['Source'] == 'facebook') & (unmatched['Category'] == 'Finance') ].sort_values(by = 'Users', ascending = False)

unmatched = unmatched.drop(['key1', 'key2'], axis=1)

final_result = pd.concat([matched,unmatched],axis = 0)

final_result['CPL'] = final_result['spend']/final_result['Users']
final_result['ROI'] =((final_result['Revenue']/final_result['spend'])-1)*100
final_result['Joining perc'] = (final_result['Joins']/final_result['Users'])*100
final_result['Conversion percent'] = (final_result['Converted']/final_result['Joins'])*100

mapped_camp = final_result[['Category','Source','Title','Comment','campaignName','adsetName','adName','spend','impressions','clicks','Users','Leads','CPL','Joins','Joining perc','Converted','Conversion percent','Revenue','ROI']]

search_campaigns = mapped_camp[(mapped_camp['Comment'].str.contains('twfu|tfal',case=False, na = False))&(mapped_camp['Comment'].str.contains('search',case=False, na = False)) ]

grouped = search_campaigns.groupby('Comment').agg({
    'Category':'first',
    'Source':'first',
    'campaignName':'first',
    'adsetName':'first',
    'adName':'first',
    'spend': 'first',         # spend is the same across rows
    'impressions': 'first',         # spend is the same across rows
    'clicks': 'first',         # spend is the same across rows
    'Users': 'sum',
    'Leads': 'sum',
    'Joins': 'sum',
    'Revenue': 'sum',
    'Converted': 'sum',
    'Title': lambda x: ', '.join(x.unique())  # optional: combine all titles
}).reset_index()
grouped

mapped_camp = mapped_camp[
    ~((mapped_camp['Comment'].str.contains('twfu|tfal', case=False, na=False)) &
    (mapped_camp['Comment'].str.contains('search', case=False, na=False)))
]

Corrected_spends = pd.concat([mapped_camp,grouped])
Corrected_spends

Corrected_spends[(Corrected_spends['Category'] == 'Finance') & (Corrected_spends['Source'] == 'facebook')]['spend'].sum()

Corrected_spends[Corrected_spends['spend'].isnull()].sort_values(by = 'Leads',ascending = False)

Corrected_spends[Corrected_spends['adName'] == 'Video_VID_00024_MC_KPSST_LP_LoFrOr']

Corrected_spends = Corrected_spends[Corrected_spends['Category'] == Category]

json_key = {
  "type": "service_account",
  "project_id": "dailyclassspend",
  "private_key_id": "634f30864f13cde3adf3f477eb481f0a79a59ab5",
  "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCuLRRia96xneqX\niIvzI6XY9yVoUZzrdEiQMyqnjTSZvVoeYITVBOImKdAxURfA8wTarTbQbQRLkhE1\nRYgAjntWFhCTJgbGpY+70+CcmW7cV53sopDwtZfGqmayOgVV1y89ltGhFrTNrwbs\nvGMmQEBXIKfTfg4a7gdPOzF5HZBuE/6l9fbpAZsq3ebx/yx5rHBk7erRKOVQgzvz\nzizgkAF3qrWhuoUxcGYcIMcpWoLO54BMPgeizMGDBGM8dFk5yz6TSJrUAl+F3hkY\n61+7CrIkwgnRbcuY+T8f+bAMXFukCFrFix6baQ3x98yG5FIiwrsFwoaT+DtbHEUK\nBfid6+mPAgMBAAECggEAFeVM3rfjcuirM7QoA79uJexLzjq+7fUxpLm0ma5Coou5\n5ypdsyRoruuAD0hmeAxqMSNSn2OqWpBY4XLa5KxaDORmiQO1blt5yjREfus66IWL\nMNLu7ZOmf4RZt1CxQwKeHwagAQ7drnhbon8RknQZTS//E7M7tV70REg+BpUU3FbU\nn4obhSMULHElHUW3/Wl1QBqt2yWuW5xNH4JIO738jGIJsPGRpIwE/13K46MKwGVf\ngxyPqqY5q0uMHDz6lWP1gfkypkQdA1J109DHFlxhVqn2cPzRxr9zR1fsRTAI0KS1\nxuQqeQYY4vXvofv+wknUF1Yva7zkVQs5M/zj3jaQYQKBgQDfwgi3cLaEtHw4f9b4\nrIaVNfvpMmRlJyp8sLe2barc6wSd/lMJKAie7JdGwG5Ga3i1RN+u8Lt0h2APj9fu\nMKe1es3LP/m+ZvUuqhG8vlIUDCA306WBDKewT9C9KY7PDDKBc/XXbWCH+wp7livd\npo94yFdciFyWO+O/+m40jeN0+wKBgQDHRhLwB4VsPR/3KhqdhJloCl1ANOKdiYHo\nCSUN5dZtBVa6yEzuEi2vjEFM/c5U5DIEZSpOXMkzlIZO3/4bVRXF+uWkIlhSqytA\nfyJhrYW1aPJJIxfLKY0dFQBGn8zS9nT933bTWBvvtEKfyQqhXdA0/MHqzcii8ude\nwxaW7ltxfQKBgQC6jntX/miekiBzttqT8Lww0ZabokEkORhqZ6h0OFFZ6bSqya2P\nl8pAPsAN5EjM+7PjZm4c3GdeS7RipSqCVByv6ZiFWmyuPAc2BePN+txSDJ4cDaES\nzGtPS+ycmnaP1/qQBg+1smLmXP8rdVRTdBhwupBj/Ok89Of5WyHhPV+kWwKBgGNS\nePPsO8M0QC94a+WhBrlFmS78bQsqU3ZswGxJ53NVWuIkHbv3kY/S72VJh8LmLkvk\nhI6kTqCH4d2EZWoIIZ8ZK8aAW9dwe1FdTKy0yVUfJsvvUtevMSeSpR2OXo+zyaIT\n9GzomJwEE8UCdmrxkGoopRq8UMlvGoUv5Z161lcdAoGAXtyr4waNBTTx/UoMZfV+\nC3Glw3M/Um1Gl1lNkDsNHlYcI7dLIuUDpfyJQYo7cVZ1zBJ1HnSut9VDjLwNY5LT\nempRP9R3jVViMvLMkHxZB85zwJlP1VLplgjFzK/lgNjY+npv/xvTHamDcGMX65w5\ngjUmGiH8OMUbnUSAqhaLC2o=\n-----END PRIVATE KEY-----\n",
  "client_email": "class-spend@dailyclassspend.iam.gserviceaccount.com",
  "client_id": "102229672555957037003",
  "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  "token_uri": "https://oauth2.googleapis.com/token",
  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
  "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/class-spend%40dailyclassspend.iam.gserviceaccount.com",
  "universe_domain": "googleapis.com"
}

import gspread
from gspread_dataframe import set_with_dataframe
from oauth2client.service_account import ServiceAccountCredentials
scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
credentials = ServiceAccountCredentials.from_json_keyfile_dict(json_key, scope)
client = gspread.authorize(credentials)

import numpy as np
import gspread
from gspread_dataframe import set_with_dataframe, get_as_dataframe

# Open the spreadsheet
spreadsheet = client.open_by_url(spreadSheet_URL)

# Define sheet name
sheet_name = f"{worksheet_name}"
stacked_df_categorized = Corrected_spends.replace([np.inf, -np.inf], np.nan).fillna(0)
try:
    worksheet = spreadsheet.worksheet(sheet_name)
except gspread.exceptions.WorksheetNotFound:
    worksheet = spreadsheet.add_worksheet(title=sheet_name, rows=100, cols=20)
def upload_chunked_dataframe(df, worksheet, chunk_size=1000):
    for start_row in range(0, len(df), chunk_size):
        chunk = df.iloc[start_row:start_row + chunk_size]
        set_with_dataframe(worksheet, chunk, row=start_row + 1, include_column_header=(start_row == 0))
upload_chunked_dataframe(stacked_df_categorized, worksheet)

worksheet.format("A:M", {
    "horizontalAlignment": "CENTER",

    "textFormat": {
      "fontSize": 11,
    }

    }
    )

    


